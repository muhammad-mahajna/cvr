{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from utils import process_file\n",
    "from CNN1DModel import CNN1DModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data loaders\n",
    "\n",
    "REMOVE_TIME_POINT = 5  # Remove the first samples from the fMRI data\n",
    "DATA_THRESHOLD = 1\n",
    "ZERO_COUNT_THRESHOLD = 43 # 10%x430\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "def load_dataset_parallel(input_dir, target_dir, num_workers=1):\n",
    "    # List all files in the directories, sorted for consistency\n",
    "    input_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(('.nii', '.nii.gz'))])\n",
    "    target_files = sorted([os.path.join(target_dir, f) for f in os.listdir(target_dir) if f.endswith(('.nii', '.nii.gz'))])\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    # Use parallel processing to load inputs and targets\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        inputs = list(executor.map(\n",
    "            process_file, \n",
    "            input_files, \n",
    "            [False] * len(input_files), \n",
    "            [REMOVE_TIME_POINT] * len(input_files), \n",
    "            [DATA_THRESHOLD] * len(input_files), \n",
    "            [ZERO_COUNT_THRESHOLD] * len(input_files)\n",
    "        ))\n",
    "        targets = list(executor.map(process_file, target_files, [True] * len(target_files)))\n",
    "\n",
    "    # Stack the loaded data\n",
    "    inputs = np.vstack([x for x in inputs if x.size > 0]).astype(np.float32)\n",
    "    targets = np.vstack([x for x in targets if x.size > 0]).astype(np.float32)\n",
    "\n",
    "    # Save original length\n",
    "    original_length = inputs.shape[0]\n",
    "\n",
    "    # Create a boolean mask for rows where all values are zero in inputs or targets\n",
    "    non_zero_mask = ~((inputs == 0).all(axis=1) | (targets == 0).all(axis=1))\n",
    "\n",
    "    # Apply the mask to filter out rows\n",
    "    inputs = inputs[non_zero_mask, :]  # Keep rows in inputs\n",
    "    targets = targets[non_zero_mask, :]  # Keep rows in targets\n",
    "\n",
    "    # Calculate the number of removed samples\n",
    "    removed_samples = original_length - inputs.shape[0]\n",
    "\n",
    "    print(f\"Final input shape: {inputs.shape}\")\n",
    "    print(f\"Final target shape: {targets.shape}\")\n",
    "    print(f\"Samples included: {inputs.shape[0]}\")\n",
    "    print(f\"Samples removed: {removed_samples}\")\n",
    "\n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Final input shape: (2187083, 430)\n",
      "Final target shape: (2187083, 1)\n",
      "Samples included: 2187083\n",
      "Samples removed: 4841653\n",
      "Loading validation data...\n",
      "Final input shape: (353280, 430)\n",
      "Final target shape: (353280, 1)\n",
      "Samples included: 353280\n",
      "Samples removed: 711680\n",
      "Loading test data...\n",
      "Final input shape: (621633, 430)\n",
      "Final target shape: (621633, 1)\n",
      "Samples included: 621633\n",
      "Samples removed: 1401791\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "\n",
    "def load_cached_data(input_file, target_file, input_dir, target_dir):\n",
    "    if os.path.exists(input_file) and os.path.exists(target_file):\n",
    "        inputs = np.load(input_file)\n",
    "        targets = np.load(target_file)\n",
    "    else:\n",
    "        inputs, targets = load_dataset_parallel(input_dir, target_dir)\n",
    "        np.save(input_file, inputs)  # Save inputs\n",
    "        np.save(target_file, targets)  # Save targets\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Common directory\n",
    "BASE_DIR = \"/Users/muhammadmahajna/workspace/research/data/cvr_est_project\"\n",
    "\n",
    "# Subdirectories\n",
    "TRAIN_INPUT_DIR = os.path.join(BASE_DIR, \"func/registered/main_data/training\")\n",
    "VAL_INPUT_DIR = os.path.join(BASE_DIR, \"func/registered/main_data/validation\")\n",
    "TEST_INPUT_DIR = os.path.join(BASE_DIR, \"func/registered/main_data/testing\")\n",
    "\n",
    "TRAIN_TARGET_DIR = os.path.join(BASE_DIR, \"CVR_MAPS/registered/training\")\n",
    "VAL_TARGET_DIR = os.path.join(BASE_DIR, \"CVR_MAPS/registered/validation\")\n",
    "TEST_TARGET_DIR = os.path.join(BASE_DIR, \"CVR_MAPS/registered/testing\")\n",
    "\n",
    "# Load and preprocess the data\n",
    "print(\"Loading training data...\")\n",
    "train_inputs, train_targets = load_cached_data(\"train_inputs.npy\", \"train_targets.npy\", TRAIN_INPUT_DIR, TRAIN_TARGET_DIR)\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "val_inputs, val_targets = load_cached_data(\"val_inputs.npy\", \"val_targets.npy\", VAL_INPUT_DIR, VAL_TARGET_DIR)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_inputs, test_targets = load_cached_data(\"test_inputs.npy\", \"test_targets.npy\", TEST_INPUT_DIR, TEST_TARGET_DIR)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TimeSeriesDataset(train_inputs, train_targets)\n",
    "val_dataset = TimeSeriesDataset(val_inputs, val_targets)\n",
    "test_dataset = TimeSeriesDataset(test_inputs, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'CNN1DModel.CNN1DModel(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m INPUT_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m435\u001b[39m \u001b[38;5;241m-\u001b[39m REMOVE_TIME_POINT \u001b[38;5;66;03m# Number of time points\u001b[39;00m\n\u001b[1;32m      3\u001b[0m LEARNING_RATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCNN1DModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'CNN1DModel.CNN1DModel(...)'?"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build model\n",
    "INPUT_SIZE = 435 - REMOVE_TIME_POINT # Number of time points\n",
    "LEARNING_RATE = 1e-3\n",
    "model = CNN1DModel(input_size=INPUT_SIZE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EPOCHS = 20\n",
    "MODEL_SAVE_PATH = \"best_model.pth\"\n",
    "PATIENCE = 5  # Stop training if validation loss doesn't improve for 5 consecutive epochs\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")  # Initialize with a large value\n",
    "    epochs_without_improvement = 0  # Counter for epochs without improvement\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"Model saved with Validation Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= PATIENCE:\n",
    "            print(f\"Early stopping triggered. No improvement for {PATIENCE} consecutive epochs.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "print(\"Evaluating the model on test data...\")\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
