{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Project libs\n",
    "from utils import process_file\n",
    "from utils import normalize_fmri_timeseries \n",
    "from CVRRegressionModel import CVRRegressionModel\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "\n",
    "# Preprocessing parameters\n",
    "REMOVE_TIME_POINT = 0         # Select which samples to remove from the data, up to the REMOVE_TIME_POINT\n",
    "SLICE_SELECT = [0, 26]        # Select which slices to include in the training\n",
    "DATA_THRESHOLD = 0            # Select data threshold. Any value in the fMRI signals lower than the threshold will be zeroed\n",
    "ZERO_COUNT_THRESHOLD = 435    # 10%x430 # Select zero-count threshold. fMRI signals with a zero-value count higher that the thresholed will be zeroed (all signal)\n",
    "REMOVE_ALL_ZERO_SAMPS = False # Remove all-zero signals from the datapool\n",
    "NORMALIZE_DATA = False        # A flag indicating wether to normalize the data or not. \n",
    "\n",
    "# Define the timseries dataset object\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, normalize=False):\n",
    "        \"\"\"\n",
    "        Dataset class for time-series data.\n",
    "\n",
    "        Args:\n",
    "            inputs (numpy.ndarray): Input data of shape (N, T) or higher dimensions (e.g., (X, Y, Z, T)).\n",
    "            targets (numpy.ndarray): Target data of shape (N, ...) or corresponding spatial dimensions.\n",
    "            normalize (bool): Whether to normalize the inputs using `normalize_fmri_timeseries`.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            self.inputs, self.scaler = normalize_fmri_timeseries(inputs)\n",
    "        else:\n",
    "            # No normalization applied\n",
    "            self.inputs = inputs\n",
    "            self.scaler = None \n",
    "        # Add channel dimension (pytorch req.)\n",
    "        self.inputs = torch.tensor(self.inputs, dtype=torch.float32).unsqueeze(1)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "    \n",
    "\n",
    "# Function for loading the data from NIFTII files\n",
    "def load_dataset_parallel(input_dir, target_dir, num_workers=1, slice_select=SLICE_SELECT):\n",
    "    \"\"\"\n",
    "    Load inputs and targets with optional slice selection using parallel processing.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Directory containing input files.\n",
    "        target_dir (str): Directory containing target files.\n",
    "        num_workers (int): Number of parallel workers.\n",
    "        slice_select (list or tuple): Range of slices to select (e.g., [4, 24]).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (inputs, targets), where both are numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    # List all files in the directories, sorted for consistency. Files could be compressed (.gz files)\n",
    "    input_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(('.nii', '.nii.gz'))])\n",
    "    target_files = sorted([os.path.join(target_dir, f) for f in os.listdir(target_dir) if f.endswith(('.nii', '.nii.gz'))])\n",
    "\n",
    "    # Use parallel processing to load inputs and targets\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        inputs = list(executor.map(\n",
    "            process_file, # Main \n",
    "            input_files, \n",
    "            [False] * len(input_files),  # is_target=False for inputs\n",
    "            [REMOVE_TIME_POINT] * len(input_files),\n",
    "            [DATA_THRESHOLD] * len(input_files),\n",
    "            [ZERO_COUNT_THRESHOLD] * len(input_files),\n",
    "            [slice_select] * len(input_files)  # Pass slice range for inputs\n",
    "        ))\n",
    "        targets = list(executor.map(\n",
    "            process_file, \n",
    "            target_files, \n",
    "            [True] * len(target_files),  # is_target=True for targets\n",
    "            [None] * len(target_files),  # remove_time_points not needed for targets\n",
    "            [0] * len(target_files),  # data_threshold not needed for targets\n",
    "            [0] * len(target_files),  # max_zeros not needed for targets\n",
    "            [slice_select] * len(target_files)  # Pass slice range for targets\n",
    "        ))\n",
    "\n",
    "    # Stack the loaded data\n",
    "    inputs = np.vstack([x for x in inputs if x.size > 0]).astype(np.float32)\n",
    "    targets = np.vstack([x for x in targets if x.size > 0]).astype(np.float32)\n",
    "\n",
    "    # Save original length\n",
    "    original_length = inputs.shape[0]\n",
    "\n",
    "    if REMOVE_ALL_ZERO_SAMPS:\n",
    "        # Create a boolean mask for rows where all values are zero in inputs or targets\n",
    "        non_zero_mask = ~((inputs == 0).all(axis=1) | (targets == 0).all(axis=1))\n",
    "\n",
    "        # Apply the mask to filter out rows\n",
    "        inputs = inputs[non_zero_mask, :]  # Keep rows in inputs\n",
    "        targets = targets[non_zero_mask, :]  # Keep rows in targets\n",
    "\n",
    "    # Calculate the number of removed samples\n",
    "    removed_samples = original_length - inputs.shape[0]\n",
    "\n",
    "    print(f\"Final input shape: {inputs.shape}\")\n",
    "    print(f\"Final target shape: {targets.shape}\")\n",
    "    print(f\"Samples included: {inputs.shape[0]}\")\n",
    "    print(f\"Samples removed: {removed_samples}\")\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "# Function to define the common directory\n",
    "def detect_base_directory():\n",
    "    possible_dirs = [\n",
    "        r\"/home/muhammad.mahajna/workspace/research/data/cvr_est_project\",  # Directory on ARC cluster\n",
    "        r\"/Users/muhammadmahajna/workspace/research/data/cvr_est_project\"   # Directory on LAPTOP - abs path\n",
    "    ] # Add your path here if the data is stored in a different location\n",
    "\n",
    "    for base_dir in possible_dirs:\n",
    "        if os.path.exists(base_dir):\n",
    "            print(f\"Using base directory: {base_dir}\")\n",
    "            return base_dir\n",
    "    # Alternatively, you can use the host name, but it works just fine as is\n",
    "\n",
    "    # Raise an error if no valid data directory is found\n",
    "    raise ValueError(\"No valid base directory found.\")\n",
    "\n",
    "# Cache the data in .npy files for speed\n",
    "def load_cached_data(input_file, target_file, input_dir, target_dir):\n",
    "    if os.path.exists(input_file) and os.path.exists(target_file):\n",
    "        inputs = np.load(input_file)\n",
    "        targets = np.load(target_file)\n",
    "    else:\n",
    "        inputs, targets = load_dataset_parallel(input_dir, target_dir)\n",
    "        np.save(input_file, inputs)  # Save inputs\n",
    "        np.save(target_file, targets)  # Save targets\n",
    "    \n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Loading of the Data \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define the base directory\n",
    "base_dir = detect_base_directory()\n",
    "\n",
    "# Subdirectories - training/validation/testing\n",
    "train_input_dir = os.path.join(base_dir, \"func/registered/main_data/training\")\n",
    "val_input_dir = os.path.join(base_dir, \"func/registered/main_data/validation\")\n",
    "test_input_dir = os.path.join(base_dir, \"func/registered/main_data/testing\")\n",
    "\n",
    "train_target_dir = os.path.join(base_dir, \"CVR_MAPS/registered/training\")\n",
    "val_target_dir = os.path.join(base_dir, \"CVR_MAPS/registered/validation\")\n",
    "test_target_dir = os.path.join(base_dir, \"CVR_MAPS/registered/testing\")\n",
    "\n",
    "# Load and preprocess the data\n",
    "print(\"Loading training data...\")\n",
    "train_inputs, train_targets = load_cached_data(\"data/train_inputs.npy\", \"data/train_targets.npy\", train_input_dir, train_target_dir)\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "val_inputs, val_targets = load_cached_data(\"data/val_inputs.npy\", \"data/val_targets.npy\", val_input_dir, val_target_dir)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_inputs, test_targets = load_cached_data(\"data/test_inputs.npy\", \"data/test_targets.npy\", test_input_dir, test_target_dir)\n",
    "\n",
    "# Create datasets using the loaded data\n",
    "train_dataset = TimeSeriesDataset(train_inputs, train_targets, normalize=NORMALIZE_DATA)\n",
    "val_dataset = TimeSeriesDataset(val_inputs, val_targets, normalize=NORMALIZE_DATA)\n",
    "test_dataset = TimeSeriesDataset(test_inputs, test_targets, normalize=NORMALIZE_DATA)\n",
    "\n",
    "# Create dataloaders from torch\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and training tools\n",
    "INPUT_SIZE = 435 - REMOVE_TIME_POINT # Number of time points\n",
    "LEARNING_RATE = 1e-3                 # Set the learning rate\n",
    "model_cnn = CVRRegressionModel()     # Define the model\n",
    "criterion = nn.MSELoss()             # Define Loss function\n",
    "optimizer_cnn = torch.optim.Adam(model_cnn.parameters(), lr=LEARNING_RATE) # Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "# Set Parameters\n",
    "MAX_EPOCHS = 10\n",
    "MODEL_SAVE_PATH = \"best_model.pth\"\n",
    "PATIENCE = 3  # Stop training if validation loss doesn't improve for 5 consecutive epochs\n",
    "GRAD_CLIP = 1.0  # Gradient clipping max norm\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device):\n",
    "    \n",
    "    model.to(device) # Move the mode to the set device\n",
    "    best_val_loss = float(\"inf\")  # Initialize with a large value\n",
    "    epochs_without_improvement = 0  # Counter for epochs without improvement\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1, verbose=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train() # set to train mode\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{MAX_EPOCHS}, Train Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Validation stage\n",
    "        model.eval() # Change the model mode to evaluation\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs, _ = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"Model saved with Validation Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= PATIENCE:\n",
    "            print(f\"Early stopping triggered. No improvement for {PATIENCE} consecutive epochs.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "print(\"Training the model...\")\n",
    "train_model(model_cnn, train_loader, val_loader, criterion, optimizer_cnn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "print(\"Evaluating the model on test data...\")\n",
    "model_cnn.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs, _ = model_cnn(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
