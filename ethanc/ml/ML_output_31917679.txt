2024-08-20 11:32:02.131067: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-20 11:32:03.210075: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-20 11:32:03.584662: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-20 11:32:03.693826: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-20 11:32:04.351204: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-20 11:32:07.309768: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/ethan.church/software/miniconda3/envs/gpu_tflow/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
starting loading in the data
finished loading in data
reshaping the data
making a dataset
getting a validation sample
finished splitting the validation
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ conv1d (Conv1D)                 │ (None, 435, 20)        │            80 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization             │ (None, 435, 20)        │            80 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d (MaxPooling1D)    │ (None, 217, 20)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_1 (Conv1D)               │ (None, 217, 40)        │         2,440 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d_1 (MaxPooling1D)  │ (None, 108, 40)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (Dropout)               │ (None, 108, 40)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_2 (Conv1D)               │ (None, 108, 80)        │         9,680 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_1           │ (None, 108, 80)        │           320 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d_2 (MaxPooling1D)  │ (None, 54, 80)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_3 (Conv1D)               │ (None, 54, 160)        │        38,560 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_2           │ (None, 54, 160)        │           640 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d_3 (MaxPooling1D)  │ (None, 27, 160)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_1 (Dropout)             │ (None, 27, 160)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_4 (Conv1D)               │ (None, 27, 320)        │       153,920 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_3           │ (None, 27, 320)        │         1,280 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d_4 (MaxPooling1D)  │ (None, 13, 320)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten (Flatten)               │ (None, 4160)           │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_2 (Dropout)             │ (None, 4160)           │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 1)              │         4,161 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 211,161 (824.85 KB)
 Trainable params: 210,001 (820.32 KB)
 Non-trainable params: 1,160 (4.53 KB)
Epoch 1/50
10117120/10117120 - 30250s - 3ms/step - loss: 0.0177 - mae: 0.0586 - mse: 0.0168 - val_loss: 0.0158 - val_mae: 0.0565 - val_mse: 0.0158
Epoch 2/50
10117120/10117120 - 30201s - 3ms/step - loss: 0.0148 - mae: 0.0574 - mse: 0.0148 - val_loss: 0.0158 - val_mae: 0.0565 - val_mse: 0.0158
Epoch 3/50
10117120/10117120 - 29939s - 3ms/step - loss: 0.0148 - mae: 0.0574 - mse: 0.0148 - val_loss: 0.0158 - val_mae: 0.0565 - val_mse: 0.0158
Epoch 4/50
10117120/10117120 - 29278s - 3ms/step - loss: 0.0148 - mae: 0.0574 - mse: 0.0148 - val_loss: 0.0158 - val_mae: 0.0565 - val_mse: 0.0158
Epoch 5/50
10117120/10117120 - 29261s - 3ms/step - loss: 0.0148 - mae: 0.0574 - mse: 0.0148 - val_loss: 0.0158 - val_mae: 0.0565 - val_mse: 0.0158
Epoch 6/50
10117120/10117120 - 29226s - 3ms/step - loss: 0.0148 - mae: 0.0574 - mse: 0.0148 - val_loss: 0.0158 - val_mae: 0.0565 - val_mse: 0.0158
Epoch 7/50
10117120/10117120 - 29330s - 3ms/step - loss: 0.0148 - mae: 0.0574 - mse: 0.0148 - val_loss: 0.0158 - val_mae: 0.0565 - val_mse: 0.0158
Epoch 8/50
10117120/10117120 - 29184s - 3ms/step - loss: 0.0148 - mae: 0.0574 - mse: 0.0148 - val_loss: 0.0158 - val_mae: 0.0565 - val_mse: 0.0158
Epoch 9/50
